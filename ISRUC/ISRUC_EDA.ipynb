{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10240345,"sourceType":"datasetVersion","datasetId":6326521},{"sourceId":211987458,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishitjakharia/isruc-eda-modelling?scriptVersionId=214549012\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Importing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfrom scipy.signal import butter, filtfilt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Setting Seed for reproducability","metadata":{}},{"cell_type":"code","source":"import random\nimport os\n\nseed_value = 42\n\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initial Graphs for class imbalance\n----\n\n```python\nevent_count = 0\n\nfor foldername, subfolders, filenames in os.walk('/kaggle/input/isruc-processed/dataset/Events'):\n    if foldername.split('/')[-1] == 'Events':\n        continue\n    event_count += len(filenames)\n    print(f\"Event: '{foldername.split('/')[-1]}' has {len(filenames)} files.\")\n\n\nfor foldername, subfolders, filenames in os.walk('/kaggle/input/isruc-processed/dataset/Non_Events'):\n    if foldername.split('/')[-1] == 'Events':\n        continue\n    event_count += len(filenames)\n    print(f\"Event: '{foldername.split('/')[-1]}' has {len(filenames)} files.\")\n\nprint(f\"Total Event count: {event_count}\")\n```","metadata":{}},{"cell_type":"markdown","source":"Now, to maintain class balance when learning a sleep stage classifier and to have sufficient amount of non-event examples we shall sample the sum of all these.\n\nAlso, we are only looking at the cases where both Experts classify the event as a non-event.","metadata":{}},{"cell_type":"code","source":"files = {\n    'l on': 141,\n    'ar': 25544,\n    'mchg': 1530,\n    'ch': 159,\n    'l out': 218,\n    'plm': 3402,\n    'rem': 213,\n    'awake': 4666,\n    'oa': 1374,\n    'ca': 362,\n    'oh': 3652,\n    'mh': 2707,\n    'lm': 823,\n    'non-events': 33531\n}\n\n# Prepare data for plotting\ncategories = list(files.keys())\nvalues = list(files.values())\n\n# Convert data to a DataFrame for seaborn\ndata = pd.DataFrame({\n    'Category': categories,\n    'Value': values\n})\n\n# Add a hue column for demonstration (e.g., splitting events into \"Events\" and \"Non-events\")\ndata['Type'] = ['Event' if cat != 'non-events' else 'Non-event' for cat in categories]\n\n# Plotting with seaborn\nplt.figure(figsize=(12, 6))\nsns.barplot(data=data, x='Category', y='Value', hue='Type', palette='coolwarm')\nplt.xlabel('Categories', fontsize=12)\nplt.ylabel('Values', fontsize=12)\nplt.title('Distribution of Event Data with Hue', fontsize=14)\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There seems to be severe class imbalance, looking at the relevant labels now","metadata":{}},{"cell_type":"code","source":"files = {\n    'oa': 1374,\n    'ca': 362,\n    'oh': 3652,\n    'mh': 2707,\n    'ch': 159\n}\n\n# Prepare data for plotting\ncategories = list(files.keys())\nvalues = list(files.values())\n\n# Convert data to a DataFrame for seaborn\ndata = pd.DataFrame({\n    'Category': categories,\n    'Value': values\n})\n\n# Add a hue column for demonstration (e.g., splitting events into \"Events\" and \"Non-events\")\ndata['Type'] = ['Event' if cat != 'non-events' else 'Non-event' for cat in categories]\n\n# Plotting with seaborn\nplt.figure(figsize=(12, 6))\nsns.barplot(data=data, x='Category', y='Value', hue='Type', palette='YlGnBu')\nplt.xlabel('Categories', fontsize=12)\nplt.ylabel('Values', fontsize=12)\nplt.title('Distribution of Event Data with Hue', fontsize=14)\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Data\n----\n\n## Sleep Stage Dataset","metadata":{}},{"cell_type":"code","source":"class Normalization(object):\n    def __init__(self, method=\"z-score\", axis=1, epsilon=1e-8):\n        \"\"\"\n        Args:\n            method (str): Normalization method ('z-score' or 'min-max').\n            axis (int): Axis along which to normalize (0: samples, 1: channels).\n            epsilon (float): Small constant to avoid division by zero.\n        \"\"\"\n        self.method = method\n        self.axis = axis\n        self.epsilon = epsilon\n\n    def __call__(self, signal):\n        \"\"\"\n        Apply normalization to the signal.\n\n        Args:\n            signal (torch.Tensor): Signal tensor of shape (n_channels, n_samples).\n        \n        Returns:\n            torch.Tensor: Normalized signal tensor of the same shape.\n        \"\"\"\n        signal_np = signal.numpy() if isinstance(signal, torch.Tensor) else signal\n\n        if self.method == \"z-score\":\n            mean = np.mean(signal_np, axis=self.axis, keepdims=True)\n            std = np.std(signal_np, axis=self.axis, keepdims=True) + self.epsilon\n            normalized_signal = (signal_np - mean) / std\n        elif self.method == \"min-max\":\n            min_val = np.min(signal_np, axis=self.axis, keepdims=True)\n            max_val = np.max(signal_np, axis=self.axis, keepdims=True) + self.epsilon\n            normalized_signal = (signal_np - min_val) / (max_val - min_val)\n        else:\n            raise ValueError(\"Unsupported normalization method. Choose 'z-score' or 'min-max'.\")\n\n        return torch.tensor(normalized_signal, dtype=torch.float32)\n\nclass BandpassFilter(object):\n    def __init__(self, lowcut, highcut, fs, order=4, padding_value=0):\n        \"\"\"\n        Args:\n            lowcut (float): Low frequency cutoff (Hz).\n            highcut (float): High frequency cutoff (Hz).\n            fs (float): Sampling frequency (Hz).\n            order (int): Order of the filter.\n            padding_value (float): Value to use for padding short signals.\n        \"\"\"\n        self.lowcut = lowcut\n        self.highcut = highcut\n        self.fs = fs\n        self.order = order\n        self.padding_value = padding_value\n        \n        # Design the filter\n        nyquist = 0.5*fs\n        self.b, self.a = butter(self.order, [self.lowcut/nyquist, self.highcut/nyquist], btype='band')\n\n    def __call__(self, signal):\n        \"\"\"\n        Apply the bandpass filter to each channel in the signal.\n\n        Args:\n            signal (tensor): Signal tensor of shape (n_channels, n_samples).\n        \n        Returns:\n            tensor: Filtered signal tensor of the same shape.\n        \"\"\"\n        filtered_signal = []\n        \n        # Apply the filter to each channel\n        for i in range(signal.shape[1]):  # Iterate over channels\n            channel_signal = signal[:, i]\n            \n            # Pad the signal if it's too short\n            if len(channel_signal) <= self.order:\n                padding_length = self.order - len(channel_signal) + 1\n                channel_signal = np.pad(channel_signal, (padding_length, padding_length), mode='constant', constant_values=self.padding_value)\n            \n            # Apply the filter to the padded signal\n            filtered_signal.append(filtfilt(self.b, self.a, channel_signal))\n\n        # Convert back to tensor\n        return torch.tensor(np.array(filtered_signal), dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SleepStageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (str): Root directory containing all folders and signal files.\n            transform (callable, optional): Optional transform to apply to the signals.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.data_info = self._prepare_file_list()\n\n    def _prepare_file_list(self):\n        \"\"\"Scan the dataset directory and prepare a list of file paths and labels.\"\"\"\n        data_info = []\n        for event_type in ['Events', 'Non_Events']:\n            folder_path = os.path.join(self.root_dir, event_type)\n            if os.path.isdir(folder_path):\n                for subfolder_name in os.listdir(folder_path):\n                    subfolder_path = os.path.join(folder_path, subfolder_name)\n                    if os.path.isdir(subfolder_path):\n                        for file_name in os.listdir(subfolder_path):\n                            # Full file path\n                            file_path = os.path.join(subfolder_path, file_name)\n                            # Extraxt sleep stage label from filename\n                            label = file_name.split('_')[3].replace('Stage', '')\n                            \n                            # Ignore errorneous labels\n                            if label not in ['w', 'n1', 'n2', 'n3', 'r']:\n                                continue\n                            \n                            data_info.append((file_path, label))\n                            \n        return data_info\n\n    def __len__(self):\n        return len(self.data_info)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get file path and label\n        file_path, label = self.data_info[idx]\n\n        # converting label to one-hot\n        label_to_one_hot = {\n            'w':  [1,0,0,0,0],\n            'n1': [0,1,0,0,0],\n            'n2': [0,0,1,0,0],\n            'n3': [0,0,0,1,0],\n            'r':  [0,0,0,0,1],\n        }\n        label = label_to_one_hot[label]\n\n        possible_columns = [\n        ['X6', 'X7', 'X8', 'SaO2'],\n        ['X6', 'X7', 'X8', 'SpO2'],\n        ['29', '30', '31', 'SaO2'],\n        ['29', '30', '31', 'SpO2']\n        ]\n        \n        # Load signal data from .npy file\n        signal = pd.read_csv(file_path)\n\n        for columns in possible_columns:\n            if all(col in signal.columns for col in columns):\n                signal = signal[columns]\n                break\n\n        signal = torch.tensor(signal.values, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.long)\n\n        if self.transform:\n            for transform in self.transform:\n                signal = transform(signal)\n\n        return signal.T, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = '/kaggle/input/isruc-processed/dataset'\ntransform = [BandpassFilter(lowcut=0.2, highcut=0.5, fs=12.5), Normalization(method=\"z-score\", axis=1)]\n\nsleep_stage_dataset = SleepStageDataset(root_dir=root_dir, transform=transform)\nsleep_stage_loader = DataLoader(sleep_stage_dataset, batch_size=1, shuffle=False)\n\ncount = 5\nfor signals, labels in sleep_stage_loader:\n    print(signals.shape)\n    print(labels)\n    if count == 0:\n        break\n    count -= 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Events classification dataset","metadata":{}},{"cell_type":"code","source":"class EventClassificationDataset(Dataset):\n    def __init__(self, root_dir, transform=None, label_mapping=None, included_classes=None):\n        \"\"\"\n        Args:\n            root_dir (str): Root directory containing all folders and signal files.\n            transform (callable, optional): Optional transform to apply to the signals.\n            included_classes (list, optional): List of classes to include. If None, include all classes.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.included_classes = included_classes\n        self.label_mapping = label_mapping\n        self.data_info = self._prepare_file_list()\n\n    def _prepare_file_list(self):\n        \"\"\"Scan the dataset directory and prepare a list of file paths and labels.\"\"\"\n        data_info = []\n\n        # Process Event files\n        upper_limit_per_event = {\n            'oa': 150,\n            'ca': 150,\n            'oh': 150,\n            'mh': 150,\n            'ch': 150\n        }\n        \n        current_count = {\n            'oa': 0,\n            'ca': 0,\n            'oh': 0,\n            'mh': 0,\n            'ch': 0\n        }\n        upper_limit_to_no_events = 300\n        \n        event_path = os.path.join(self.root_dir, 'Events')\n        if os.path.isdir(event_path):\n            for label_folder in os.listdir(event_path):\n                if self.included_classes and label_folder not in self.included_classes:\n                    continue\n\n                label_path = os.path.join(event_path, label_folder)\n                if os.path.isdir(label_path):\n                    for file_name in os.listdir(label_path):\n                        # Full file path\n                        file_path = os.path.join(label_path, file_name)\n\n                        # Append (file_path, label) tuple\n                        if upper_limit_per_event[label_folder] != -1 and current_count[label_folder] == upper_limit_per_event[label_folder]:\n                            continue\n                        else:\n                            current_count[label_folder] += 1\n                        \n                        data_info.append((file_path, label_folder))\n\n        # Process Non Event files\n        non_event_path = os.path.join(self.root_dir, 'Non_Events')\n        if os.path.isdir(non_event_path):\n            for subfolder_name in os.listdir(non_event_path):\n                subfolder_path = os.path.join(non_event_path, subfolder_name)\n                if os.path.isdir(subfolder_path):\n                    for file_name in os.listdir(subfolder_path):\n                        # Full file path\n                        file_path = os.path.join(subfolder_path, file_name)\n\n                        # Label for non-events is \"no_event\"\n                        if not self.included_classes or 'no_event' in self.included_classes:\n                            data_info.append((file_path, 'no_event'))\n\n                        if upper_limit_to_no_events == 0:\n                            break\n                        upper_limit_to_no_events -= 1\n\n        return data_info\n\n    def __len__(self):\n        return len(self.data_info)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get file path and label\n        file_path, label = self.data_info[idx]\n\n        # Convert label to one-hot encoding\n        unique_labels = self.included_classes if self.included_classes else ['ar', 'awake', 'ca', 'ch', 'l on', 'l out', \n                                                                             'lm', 'mchg', 'mh', 'oa', 'oh', 'plm', 'rem', \n                                                                             'no_event']\n        label_to_one_hot = {label: [1 if i == idx else 0 for i in range(len(unique_labels))] \n                            for idx, label in enumerate(unique_labels)}\n        label = label_to_one_hot[label]\n\n        possible_columns = [\n        ['X6', 'X7', 'X8', 'SaO2'],\n        ['X6', 'X7', 'X8', 'SpO2'],\n        ['29', '30', '31', 'SaO2'],\n        ['29', '30', '31', 'SpO2']\n        ]\n        \n        signal = pd.read_csv(file_path)\n    \n        for columns in possible_columns:\n            if all(col in signal.columns for col in columns):\n                signal = signal[columns]\n                break\n        \n        signal = torch.tensor(signal.values, dtype=torch.float32)\n        if self.label_mapping:\n            label = self.label_mapping[np.argmax(label)]\n        label = torch.tensor(label, dtype=torch.long)\n        \n        if self.transform:\n            for transform in self.transform:\n                signal = transform(signal)\n\n        return signal.T, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = '/kaggle/input/isruc-processed/dataset'\nincluded_classes = ['ca', 'oa', 'oh', 'mh', 'ch', 'no_event']\ntransform = [BandpassFilter(lowcut=0.2, highcut=0.5, fs=12.5), Normalization(method='z-score', axis=1)]\n\nevent_classification_dataset = EventClassificationDataset(root_dir=root_dir, transform=transform, included_classes=included_classes)\nevent_classification_loader = DataLoader(event_classification_dataset, batch_size=1, shuffle=True)\n\ncount = 3\nfor signals, labels in event_classification_loader:\n    print(included_classes[np.argmax(labels)])\n    print('-'*100)\n    if count == 0:\n        break\n    count -= 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modelling\n----\n\n## Loss Function","metadata":{}},{"cell_type":"code","source":"def kl_divergence_loss(preds, targets):\n    targets = targets.float()\n    \n    preds = F.softmax(preds, dim=-1)\n    targets = F.softmax(targets, dim=-1)\n    loss = F.kl_div(preds.log(), targets, reduction='batchmean')\n    return loss\n\ndef mse_loss(preds, targets):\n    # Ensure that the predictions and targets are one-hot encoded\n    preds = F.softmax(preds, dim=-1)\n    return F.mse_loss(preds, targets)\n\n# Focal Loss\ndef focal_loss(preds, targets, alpha=0.25, gamma=2.0):\n    \"\"\"\n    Focal Loss for multi-class classification.\n    \n    Arguments:\n    preds -- the raw logits from the model (shape: [batch_size, num_classes])\n    targets -- the true labels (shape: [batch_size])\n    alpha -- balancing factor for class imbalances (default is 0.25)\n    gamma -- focusing parameter (default is 2.0)\n    \n    Returns:\n    loss -- the computed focal loss\n    \"\"\"\n    # Apply softmax to get probabilities\n    preds = F.softmax(preds, dim=-1)\n    \n    # Convert targets to one-hot encoding\n    targets_one_hot = targets\n    \n    # Cross-entropy loss\n    ce_loss = F.cross_entropy(preds, targets, reduction='none')\n    \n    # Get the predicted probability for the correct class\n    pt = torch.exp(-ce_loss)\n    \n    # Compute focal loss\n    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n    \n    return focal_loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer and Learning Rate","metadata":{}},{"cell_type":"code","source":"def warmup_lr(epoch, step_size, warmup_epochs=5, gamma=0.1):\n    if epoch < warmup_epochs:\n        return (epoch + 1) / warmup_epochs\n    \n    else:\n        epoch_since_warmup = epoch - warmup_epochs\n        return gamma ** (epoch_since_warmup // step_size)\n\nWARMUP_EPOCHS = 14\nGAMMA = 0.1\nSTEP_SIZE = 20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plotting Train and Test Acc and Loss","metadata":{}},{"cell_type":"code","source":"# Plot history\ndef plot_history(history):\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    plt.figure(figsize=(12, 4))\n\n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], label='Train Loss')\n    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss History')\n\n    # Accuracy plot\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_acc'], label='Train Accuracy')\n    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy History')\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Confusion Matrix Function","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(model, val_loader, class_names, device=\"cuda\"):\n    \"\"\"\n    Evaluates the model and plots a confusion matrix with a custom color map.\n\n    Arguments:\n    - model: Trained PyTorch model.\n    - val_loader: DataLoader for validation dataset.\n    - class_names: List of class names for the confusion matrix.\n    - device: 'cuda' or 'cpu'.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient computation\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, dim=1)  # Predicted classes\n            \n            # If labels are one-hot encoded, convert them to class indices\n            if len(labels.shape) > 1 and labels.size(1) > 1:\n                labels = torch.argmax(labels, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(all_labels, all_preds, labels=np.arange(len(class_names)))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n\n    # Plot the confusion matrix with customization\n    fig, ax = plt.subplots(figsize=(10, 8))  # Set larger figure size\n    disp.plot(cmap=\"YlGnBu\", ax=ax, colorbar=True)  # Use \"cividis\" colormap and add colorbar\n    plt.title(\"Confusion Matrix\", fontsize=16)\n    plt.xticks(fontsize=12, rotation=45)\n    plt.yticks(fontsize=12)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.model_selection import LeaveOneOut\nfrom collections import defaultdict\n\n\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    criterion=None,\n    optimizer=None,\n    scheduler=None,\n    epochs=20,\n    device=\"cpu\",\n    early_stopping=False,\n    patience=20,\n    min_delta=1e-3,\n    save_path=\"best_model.pth\",\n    verbose=True\n):\n    \"\"\"\n    Train a PyTorch model with optional early stopping and learning rate scheduling.\n\n    Args:\n        model (torch.nn.Module): The model to train.\n        train_loader (DataLoader): DataLoader for the training dataset.\n        val_loader (DataLoader): DataLoader for the validation dataset.\n        criterion (callable, optional): Loss function. Default is F.mse_loss.\n        optimizer (torch.optim.Optimizer): Optimizer for training.\n        scheduler (callable, optional): Learning rate scheduler.\n        epochs (int, optional): Number of training epochs. Default is 20.\n        device (str, optional): Device to train on (\"cpu\" or \"cuda\"). Default is \"cpu\".\n        early_stopping (bool, optional): Whether to enable early stopping. Default is False.\n        patience (int, optional): Patience for early stopping. Default is 20.\n        min_delta (float, optional): Minimum delta for early stopping. Default is 1e-3.\n        save_path (str, optional): Path to save the best model. Default is 'best_model.pth'.\n        verbose (bool, optional): Whether to print training progress. Default is True.\n\n    Returns:\n        dict: History of training and validation loss and accuracy.\n    \"\"\"\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n\n    best_loss = np.inf\n    best_acc = -np.inf\n    patience_counter = 0\n    best_model_state = None\n\n    if criterion is None:\n        criterion = F.mse_loss\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct = 0.0, 0\n\n        # Training phase\n        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", unit=\"batch\", disable=not verbose) as tepoch:\n            for inputs, targets in tepoch:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets.float())\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.item() * inputs.size(0)\n                train_correct += (outputs.argmax(1) == targets.argmax(1)).sum().item()\n\n                if verbose:\n                    tepoch.set_postfix(loss=loss.item())\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss /= len(train_loader.dataset)\n        train_acc = train_correct / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        with torch.no_grad():\n            with tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\", unit=\"batch\", disable=not verbose) as vepoch:\n                for inputs, targets in vepoch:\n                    inputs = inputs.to(device)\n                    targets = targets.to(device)\n\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets.float())\n                    val_loss += loss.item() * inputs.size(0)\n                    val_correct += (outputs.argmax(1) == targets.argmax(1)).sum().item()\n\n                    if verbose:\n                        vepoch.set_postfix(loss=loss.item())\n\n        val_loss /= len(val_loader.dataset)\n        val_acc = val_correct / len(val_loader.dataset)\n\n        # Save history\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_acc\"].append(val_acc)\n\n        if verbose:\n            print(\n                f\"Epoch {epoch + 1}/{epochs} - \"\n                f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n            )\n\n        # Save the best model\n        if best_acc < val_acc - min_delta:\n            best_loss = val_loss\n            best_acc = val_acc\n            patience_counter = 0\n            best_model_state = model.state_dict()\n            if verbose:\n                print(f\"New best model found at epoch {epoch + 1} with val_loss: {val_loss:.4f}\")\n        else:\n            patience_counter += 1\n\n        # Early stopping\n        if early_stopping and patience_counter >= patience:\n            if verbose:\n                print(\"Early stopping triggered!\")\n            break\n\n    # Save the best model at the end of training\n    if best_model_state is not None:\n        torch.save(best_model_state, save_path)\n        if verbose:\n            print(f\"Best model saved to {save_path} with val_loss: {best_loss:.4f} and val_acc: {best_acc:.4f}\")\n\n    return history\n\ndef train_with_loocv(\n    model_class,\n    model_params,\n    dataset,\n    batch_size,\n    optimizer_class,\n    optimizer_params,\n    shuffle=True,\n    criterion=None,\n    scheduler_class=None,\n    scheduler_params=None,\n    epochs=20,\n    device=\"cpu\",\n    early_stopping=False,\n    patience=20,\n    min_delta=1e-3,\n    save_path=\"best_model.pth\",\n    verbose=True\n):\n    \"\"\"\n    Train a PyTorch model using Leave-One-Out Cross-Validation (LOOCV), grouped by patient-session.\n\n    Args:\n        model_class (torch.nn.Module): The model class to instantiate for each fold.\n        model_params (dict): Parameters for the model initialization.\n        dataset (Dataset): The dataset to perform LOOCV on.\n        batch_size (int): The batch size for the train loader.\n        optimizer_class (torch.optim.Optimizer): Optimizer class.\n        optimizer_params (dict): Parameters to initialize the optimizer.\n        shuffle (bool): Whether to shuffle the train dataset or not. Default is True.\n        criterion (callable, optional): Loss function. Default is `mse_loss`.\n        scheduler_class (callable, optional): Learning rate scheduler class. Default is None.\n        scheduler_params (dict, optional): Parameters for the scheduler. Default is None.\n        epochs (int, optional): Number of epochs for training. Default is 20.\n        device (str, optional): Device to train on (\"cpu\" or \"cuda\"). Default is \"cpu\".\n        early_stopping (bool, optional): Whether to enable early stopping. Default is False.\n        patience (int, optional): Patience for early stopping. Default is 20.\n        min_delta (float, optional): Minimum delta for early stopping. Default is 1e-3.\n        save_path (str, optional): Path to save the best model for each fold. Default is 'best_model.pth'.\n        verbose (bool, optional): Whether to print training progress. Default is True.\n\n    Returns:\n        list: A list of histories for each fold.\n    \"\"\"\n    # Group data by patient-session based on filename\n    patient_session_groups = defaultdict(list)\n\n    for idx, (_, label) in enumerate(dataset.data_info):\n        filename = dataset.data_info[idx][0]\n        filename = filename.split(\"/\")[-1]\n        patient_session_key = \"_\".join(filename.split(\"_\")[:2])  # e.g., \"S1_p_1\"\n        patient_session_groups[patient_session_key].append(idx)\n\n    # Prepare LOOCV splits based on patient-session groups\n    group_keys = list(patient_session_groups.keys())\n    loo = LeaveOneOut()\n    histories = []\n\n    for fold, (train_group_idx, val_group_idx) in enumerate(loo.split(group_keys)):\n        if verbose:\n            print(f\"\\nFold {fold + 1}/{len(group_keys)}\")\n\n        # Get train and validation indices from groups\n        train_idx = [idx for group_idx in train_group_idx for idx in patient_session_groups[group_keys[group_idx]]]\n        val_idx = [idx for group_idx in val_group_idx for idx in patient_session_groups[group_keys[group_idx]]]\n\n        train_subset = torch.utils.data.Subset(dataset, train_idx)\n        val_subset = torch.utils.data.Subset(dataset, val_idx)\n\n        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=shuffle)\n        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n\n        model = model_class(**model_params).to(device)\n        optimizer = optimizer_class(model.parameters(), **optimizer_params)\n        scheduler = None\n        if scheduler_class is not None:\n            scheduler = scheduler_class(optimizer, **scheduler_params)\n        \n        # Train the model for the current fold\n        history = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            epochs=epochs,\n            device=device,\n            early_stopping=early_stopping,\n            patience=patience,\n            min_delta=min_delta,\n            save_path=f\"{save_path}_fold_{fold+1}.pth\",\n            verbose=verbose,\n        )\n\n        # Append history for this fold\n        histories.append(history)\n\n    return histories","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models\n----\n### Bi-LSTM + Attention","metadata":{}},{"cell_type":"code","source":"class AdaptiveRNN(nn.Module):\n    def __init__(self, n_timesteps, n_features, n_outputs):\n        super(AdaptiveRNN, self).__init__()\n\n        # Bi-directional LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=n_features,\n            hidden_size=256,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.batch_norm = nn.BatchNorm1d(256 * 2)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n\n        # Attention mechanism\n        self.attention_dense = nn.Linear(256 * 2, 1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(256 * 2, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, n_outputs)\n\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # LSTM output\n        lstm_out, _ = self.lstm(x)  # lstm_out shape: (batch_size, timesteps, 128*2)\n        \n        batch_size, n_timesteps, n_features = lstm_out.size()\n        lstm_out = lstm_out.reshape(batch_size * n_timesteps, n_features)\n        lstm_out = self.batch_norm(lstm_out)\n        lstm_out = lstm_out.reshape(batch_size, n_timesteps, n_features)\n        \n        # Attention mechanism\n        attention_scores = torch.sigmoid(self.attention_dense(lstm_out))  # (batch_size, timesteps, 1)\n        attention_scores = lstm_out * attention_scores  # Weighted timesteps\n\n        # Global average pooling\n        pooled = attention_scores.mean(dim=1)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(pooled))\n        x = self.batch_norm1(x)\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return F.softmax(x, dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model parameters\nn_timesteps = 30 * 12.5\nn_features = 4\nn_outputs = 3\n\n# Create the model\nmodel = AdaptiveRNN(n_timesteps=n_timesteps, n_features=n_features, n_outputs=n_outputs).to(device)\nsummary(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```python\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nT_MAX = 50\nETA_MIN = 1e-6\nEPOCHS = 10\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_class = AdaptiveRNN\nmodel_params = {\"n_timesteps\": 375, \"n_features\": 4, \"n_outputs\": 6}\n\n# Call train_with_loocv\n# history = train_with_loocv(\n#     model_class=model_class,\n#     model_params=model_params,\n#     dataset=event_classification_dataset,\n#     batch_size=BATCH_SIZE,\n#     criterion=kl_divergence_loss,\n#     optimizer_class=torch.optim.Adam,\n#     optimizer_params={\"lr\": LEARNING_RATE},\n#     scheduler_class=torch.optim.lr_scheduler.CosineAnnealingLR,\n#     scheduler_params={\"T_max\": T_MAX, \"eta_min\": ETA_MIN},\n#     epochs=EPOCHS,\n#     device=DEVICE,\n#     early_stopping=False,  # Set to True if you want early stopping\n#     patience=20,  # Adjust patience if early stopping is enabled\n#     verbose=True\n# )\n```","metadata":{"execution":{"iopub.status.busy":"2024-12-24T05:20:24.539609Z","iopub.execute_input":"2024-12-24T05:20:24.540408Z","iopub.status.idle":"2024-12-24T05:20:24.545247Z","shell.execute_reply.started":"2024-12-24T05:20:24.540373Z","shell.execute_reply":"2024-12-24T05:20:24.54438Z"}}},{"cell_type":"code","source":"root_dir = '/kaggle/input/isruc-processed/dataset'\nincluded_classes = ['ca', 'oa', 'oh', 'mh', 'ch', 'no_event']\ntransform = [BandpassFilter(lowcut=0.2, highcut=0.5, fs=12.5), Normalization(method='min-max', axis=1)]\n\n# label_mapping = {\n#     'ca': 'apnea',\n#     'oa': 'apnea',\n#     'ch': 'apnea',\n#     'oh': 'hypopnea',\n#     'mh': 'hypopnea',\n#     'no_event': 'no_event'\n# }\n\nlabel_mapping = [[1,0,0], [1,0,0], [1,0,0],\n                 [0,1,0], [0,1,0],\n                 [0,0,1]\n                ]\n\n# Create an instance of the SleepStageDataset\nevent_classification_dataset = EventClassificationDataset(root_dir=root_dir, transform=transform, label_mapping=label_mapping, included_classes=included_classes)\n\npatient_session_groups = defaultdict(list)\n\nfor idx, (_, label) in enumerate(event_classification_dataset.data_info):\n    filename = event_classification_dataset.data_info[idx][0]\n    filename = filename.split(\"/\")[-1]\n    patient_session_key = \"_\".join(filename.split(\"_\")[:2])  # e.g., \"S1_p_1\"\n    patient_session_groups[patient_session_key].append(idx)\n\ngroup_keys = list(patient_session_groups.keys())\ntrain_group_keys, test_group_keys = train_test_split(\n    group_keys, test_size=0.1, random_state=42\n)\n\n# Get train and test indices from groups\ntrain_idx = [idx for key in train_group_keys for idx in patient_session_groups[key]]\ntest_idx = [idx for key in test_group_keys for idx in patient_session_groups[key]]\n\n# Create train and test subsets\ntrain_subset = torch.utils.data.Subset(event_classification_dataset, train_idx)\ntest_subset = torch.utils.data.Subset(event_classification_dataset, test_idx)\n\n# Create data loaders\nbatch_size = 256\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Number of training samples: {len(train_subset)}\")\nprint(f\"Number of testing samples: {len(test_subset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n\nhistory = train_model(\n    model = model,\n    train_loader=train_loader,\n    val_loader=test_loader,\n    criterion=focal_loss,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    epochs=5,\n    device=device,\n    early_stopping=False,\n    patience=20,\n    min_delta=1e-3,\n    save_path=\"best_model.pth\",\n    verbose=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(model, test_loader, class_names=['apnea', 'hypopnea', 'no events'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Log\n----\n\n1) **LOOCV**:\n\nBi-LSTM + Attention Model + Cross Entropy + Complete Dataset with undersampled `no_events` -> 50% Train, 28% Val\n\nBi-LSTM + Attention Model + KL Div Loss + Complete Dataset with undersampled `no_events` -> 50% Train, 37% Val\n\nBi-LSTM + Attention Model + KL Div Loss + UnderSampled Dataset -> 57.57% Train, 28.81% Val\n\n2) **Train-Test Split using PatientID**:\n\nBi-LSTM + Attention Model + KL Div Loss + UnderSampled Dataset -> 52.41% Train, 51.58% Val","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}