{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:56:55.778587Z","iopub.execute_input":"2024-12-26T10:56:55.779025Z","iopub.status.idle":"2024-12-26T10:56:56.167868Z","shell.execute_reply.started":"2024-12-26T10:56:55.778984Z","shell.execute_reply":"2024-12-26T10:56:56.166784Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/isurc-signal-features/Dataset/test_labels.csv\n/kaggle/input/isurc-signal-features/Dataset/train_labels.csv\n/kaggle/input/isurc-signal-features/Dataset/train_features.csv\n/kaggle/input/isurc-signal-features/Dataset/test_features.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = pd.read_csv(\"/kaggle/input/isurc-signal-features/Dataset/train_features.csv\"), pd.read_csv(\"/kaggle/input/isurc-signal-features/Dataset/test_features.csv\"), pd.read_csv(\"/kaggle/input/isurc-signal-features/Dataset/train_labels.csv\"), pd.read_csv(\"/kaggle/input/isurc-signal-features/Dataset/test_labels.csv\")\n\n# Check shapes\nprint(f\"Training Features Shape: {X_train.shape}\")\nprint(f\"Training Labels Shape: {y_train.shape}\")\nprint(f\"Testing Features Shape: {X_test.shape}\")\nprint(f\"Testing Labels Shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:56:57.330605Z","iopub.execute_input":"2024-12-26T10:56:57.331051Z","iopub.status.idle":"2024-12-26T10:57:01.177847Z","shell.execute_reply.started":"2024-12-26T10:56:57.331025Z","shell.execute_reply":"2024-12-26T10:57:01.176868Z"}},"outputs":[{"name":"stdout","text":"Training Features Shape: (1214, 624)\nTraining Labels Shape: (1214, 1)\nTesting Features Shape: (252, 624)\nTesting Labels Shape: (252, 1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 6 Class Classification","metadata":{}},{"cell_type":"code","source":"# Train LightGBM Classifier\nlgbm_model = LGBMClassifier(n_estimators=100, random_state=42)\nlgbm_model.fit(X_train, y_train)\n\n# Evaluate\ny_pred_lgbm = lgbm_model.predict(X_test)\nprint(\"LightGBM Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\nprint(classification_report(y_test, y_pred_lgbm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:57:03.221330Z","iopub.execute_input":"2024-12-26T10:57:03.221733Z","iopub.status.idle":"2024-12-26T10:57:18.281643Z","shell.execute_reply.started":"2024-12-26T10:57:03.221699Z","shell.execute_reply":"2024-12-26T10:57:18.280627Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007519 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 133955\n[LightGBM] [Info] Number of data points in the train set: 1214, number of used features: 570\n[LightGBM] [Info] Start training from score -2.383177\n[LightGBM] [Info] Start training from score -2.145849\n[LightGBM] [Info] Start training from score -2.374288\n[LightGBM] [Info] Start training from score -2.825010\n[LightGBM] [Info] Start training from score -2.457285\n[LightGBM] [Info] Start training from score -0.592907\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nLightGBM Accuracy: 0.7698412698412699\n              precision    recall  f1-score   support\n\n           0       0.72      0.59      0.65        22\n           1       0.25      0.07      0.11        14\n           2       0.00      0.00      0.00         9\n           3       0.67      0.14      0.24        28\n           4       0.00      0.00      0.00         0\n           5       0.81      0.98      0.89       179\n\n    accuracy                           0.77       252\n   macro avg       0.41      0.30      0.31       252\nweighted avg       0.73      0.77      0.72       252\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Train XGBoost Classifier\nxgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_model.fit(X_train, y_train)\n\n# Evaluate\ny_pred_xgb = xgb_model.predict(X_test)\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:58:17.051053Z","iopub.execute_input":"2024-12-26T10:58:17.051443Z","iopub.status.idle":"2024-12-26T10:58:31.429983Z","shell.execute_reply.started":"2024-12-26T10:58:17.051411Z","shell.execute_reply":"2024-12-26T10:58:31.428818Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.7341269841269841\n              precision    recall  f1-score   support\n\n           0       0.69      0.41      0.51        22\n           1       0.17      0.14      0.15        14\n           2       0.00      0.00      0.00         9\n           3       0.00      0.00      0.00        28\n           4       0.00      0.00      0.00         0\n           5       0.79      0.97      0.87       179\n\n    accuracy                           0.73       252\n   macro avg       0.27      0.25      0.26       252\nweighted avg       0.63      0.73      0.67       252\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 3 Class Classification","metadata":{}},{"cell_type":"code","source":"# 0, 1 -> 0; 2, 3, 4 -> 1; 5-> 3\ndef map_values(value):\n    if value in [0, 1]:\n        return 0\n    elif value in [2, 3, 4]:\n        return 1\n    elif value == 5:\n        return 2\n\n# Apply the mapping\ny_train_3_class = y_train.map(map_values)\ny_test_3_class = y_test.map(map_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:04:24.104326Z","iopub.execute_input":"2024-12-26T11:04:24.104721Z","iopub.status.idle":"2024-12-26T11:04:24.112073Z","shell.execute_reply.started":"2024-12-26T11:04:24.104691Z","shell.execute_reply":"2024-12-26T11:04:24.111050Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Train LightGBM Classifier\nlgbm_model = LGBMClassifier(n_estimators=100, random_state=42)\nlgbm_model.fit(X_train, y_train_3_class)\n\n# Evaluate\ny_pred_lgbm = lgbm_model.predict(X_test)\nprint(\"LightGBM Accuracy:\", accuracy_score(y_test_3_class, y_pred_lgbm))\nprint(classification_report(y_test_3_class, y_pred_lgbm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:04:57.765580Z","iopub.execute_input":"2024-12-26T11:04:57.765970Z","iopub.status.idle":"2024-12-26T11:05:06.061571Z","shell.execute_reply.started":"2024-12-26T11:04:57.765928Z","shell.execute_reply":"2024-12-26T11:05:06.060569Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 133955\n[LightGBM] [Info] Number of data points in the train set: 1214, number of used features: 570\n[LightGBM] [Info] Start training from score -1.564342\n[LightGBM] [Info] Start training from score -1.435249\n[LightGBM] [Info] Start training from score -0.592907\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nLightGBM Accuracy: 0.8253968253968254\n              precision    recall  f1-score   support\n\n           0       1.00      0.58      0.74        36\n           1       0.61      0.38      0.47        37\n           2       0.83      0.97      0.89       179\n\n    accuracy                           0.83       252\n   macro avg       0.81      0.64      0.70       252\nweighted avg       0.82      0.83      0.81       252\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Train XGBoost Classifier\nxgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_model.fit(X_train, y_train_3_class)\n\n# Evaluate\ny_pred_xgb = xgb_model.predict(X_test)\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test_3_class, y_pred_xgb))\nprint(classification_report(y_test_3_class, y_pred_xgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:05:31.994826Z","iopub.execute_input":"2024-12-26T11:05:31.995170Z","iopub.status.idle":"2024-12-26T11:05:41.486588Z","shell.execute_reply.started":"2024-12-26T11:05:31.995142Z","shell.execute_reply":"2024-12-26T11:05:41.485362Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.7976190476190477\n              precision    recall  f1-score   support\n\n           0       1.00      0.61      0.76        36\n           1       0.47      0.19      0.27        37\n           2       0.80      0.96      0.87       179\n\n    accuracy                           0.80       252\n   macro avg       0.76      0.59      0.63       252\nweighted avg       0.78      0.80      0.77       252\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 2 Class Classification","metadata":{}},{"cell_type":"code","source":"# 0, 1 -> 0; 2, 3, 4 -> 1; 5-> 3\ndef map_values(value):\n    if value in [0, 1, 2, 3, 4]:\n        return 0\n    elif value in [5]:\n        return 1\n    \n# Apply the mapping\ny_train_2_class = y_train.map(map_values)\ny_test_2_class = y_test.map(map_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:06:33.893233Z","iopub.execute_input":"2024-12-26T11:06:33.893658Z","iopub.status.idle":"2024-12-26T11:06:33.901274Z","shell.execute_reply.started":"2024-12-26T11:06:33.893625Z","shell.execute_reply":"2024-12-26T11:06:33.900030Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Train LightGBM Classifier\nlgbm_model = LGBMClassifier(n_estimators=100, random_state=42)\nlgbm_model.fit(X_train, y_train_2_class)\n\n# Evaluate\ny_pred_lgbm = lgbm_model.predict(X_test)\nprint(\"LightGBM Accuracy:\", accuracy_score(y_test_2_class, y_pred_lgbm))\nprint(classification_report(y_test_2_class, y_pred_lgbm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:06:46.859863Z","iopub.execute_input":"2024-12-26T11:06:46.860193Z","iopub.status.idle":"2024-12-26T11:06:50.271159Z","shell.execute_reply.started":"2024-12-26T11:06:46.860167Z","shell.execute_reply":"2024-12-26T11:06:50.269972Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 671, number of negative: 543\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 133955\n[LightGBM] [Info] Number of data points in the train set: 1214, number of used features: 570\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.552718 -> initscore=0.211660\n[LightGBM] [Info] Start training from score 0.211660\nLightGBM Accuracy: 0.8134920634920635\n              precision    recall  f1-score   support\n\n           0       0.72      0.58      0.64        73\n           1       0.84      0.91      0.87       179\n\n    accuracy                           0.81       252\n   macro avg       0.78      0.74      0.76       252\nweighted avg       0.81      0.81      0.81       252\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Train XGBoost Classifier\nxgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_model.fit(X_train, y_train_2_class)\n\n# Evaluate\ny_pred_xgb = xgb_model.predict(X_test)\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test_2_class, y_pred_xgb))\nprint(classification_report(y_test_2_class, y_pred_xgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T11:07:09.625408Z","iopub.execute_input":"2024-12-26T11:07:09.625787Z","iopub.status.idle":"2024-12-26T11:07:12.884182Z","shell.execute_reply.started":"2024-12-26T11:07:09.625757Z","shell.execute_reply":"2024-12-26T11:07:12.882963Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.7936507936507936\n              precision    recall  f1-score   support\n\n           0       0.68      0.55      0.61        73\n           1       0.83      0.89      0.86       179\n\n    accuracy                           0.79       252\n   macro avg       0.75      0.72      0.73       252\nweighted avg       0.79      0.79      0.79       252\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}